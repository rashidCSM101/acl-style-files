\documentclass[11pt]{article}

\usepackage[review]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\title{A Generic Architecture for OCR of Arabic-Alphabet Based Languages Using Nastaleeq: A Graph Attention Approach}

\author{Sayed Rashid Ali Shah \\
  Department of Computer Science \\
  Quaid-i-Azam University, Islamabad \\
  \texttt{srashid.22413012@cs.qau.edu.pk} \\\And
  Second Author \\
  Affiliation \\
  \\
  \texttt{} \\}

\begin{document}
\maketitle

\begin{abstract}
Optical character recognition for Arabic-alphabet based languages---particularly those rendered in the Nastaleeq script---remains a surprisingly stubborn problem. The script's cursive flow, vertical stacking of characters, and spatially displaced diacritics push most conventional OCR pipelines to their limits. Existing approaches typically pair a CNN with an LSTM and treat text as a flat 1D sequence. But Nastaleeq isn't flat. It's deeply two-dimensional and structurally hierarchical. In this paper, we propose a hybrid pipeline that brings in a Graph Attention Network (GAT) between the CNN feature extractor and the Bi-LSTM sequence decoder. Each detected ligature is treated as a node in a spatial graph, and the GAT learns---through attention---which ligature relationships matter most. We describe the full architecture, walk through the graph construction logic, and present a prototype evaluation on a subset of the UPTI dataset. Early results are promising: our approach cuts character error rate by a notable margin compared to a standard CNN-LSTM baseline. To the best of our knowledge, this is the first application of graph attention mechanisms specifically designed for Nastaleeq-style OCR.
\end{abstract}

%--------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}

Here's a question that sounds simple but really isn't: why is reading Urdu text from an image still so hard for machines? English OCR has been more or less solved for years---Tesseract handles it, commercial tools handle it, and most people don't think twice about scanning a document. But try doing the same thing with Nastaleeq, the calligraphic style used for Urdu, Persian, Pashto, and several other languages, and you run into a wall pretty quickly.

The core issue is structural. Nastaleeq isn't just cursive---it's \emph{vertically stacked} cursive. Characters pile on top of each other, dots (called \emph{nuktay} in Urdu) float above or below their associated base character at unpredictable offsets, and the shape of any individual letter changes depending on what comes before and after it. A ``bay'' connected to a ``yay'' looks completely different from a standalone ``bay.'' This kind of context-sensitivity is hard to capture when you're processing text as a simple left-to-right sequence.

Most existing Nastaleeq OCR systems follow a fairly standard recipe: extract visual features with a CNN, pass them through an LSTM (often bidirectional), and decode using CTC \citep{Graves2006}. This pipeline has worked reasonably well for Naskh-style Arabic \citep{Bluche2017}, but Nastaleeq's unique properties---especially the vertical overlap and displaced dots---cause these systems to struggle. The CNN sees local patches just fine, but it doesn't really understand how those patches relate to each other spatially.

That's where graph neural networks come in. Instead of treating every ligature region as an independent entry in a flat sequence, we represent them as nodes in a graph. The spatial relationships between them---who's next to whom, who's stacked on top, which dot belongs to which base character---become edges. Once you've got a graph, you can apply attention mechanisms. Specifically, we use Graph Attention Networks \citep{Velickovic2018}, which let the model learn \emph{how much} each neighboring node matters. A nukta node should attend strongly to its base character. Two horizontally adjacent ligatures need to know about each other for reading order. And stacked characters need vertical connectivity that a 1D model just can't provide.

Our main contributions are:

\begin{itemize}
    \item A hybrid OCR architecture combining CNN feature extraction, graph-based structural modeling via GAT, and Bi-LSTM sequence decoding for Nastaleeq script recognition.
    \item A graph construction scheme tailored to Nastaleeq, with three distinct edge types: horizontal (reading order), vertical (stacking), and nukta-to-base connections.
    \item Prototype evaluation showing that GAT-based structural reasoning improves recognition accuracy over a standard CNN-LSTM baseline.
\end{itemize}

%--------------------------------------------------------------------
\section{Related Work}
\label{sec:related}

\subsection{OCR for Arabic-Script Languages}

Work on Arabic script OCR goes back decades, though most of it has focused on Naskh---the simpler, more horizontal writing style common in Arabic text. \citet{Naz2016} provided a fairly comprehensive survey of Urdu OCR methods, noting that Nastaleeq remained a ``significantly harder'' problem compared to Naskh. Early approaches relied on hand-crafted features and template matching, which worked for isolated characters but fell apart on connected text.

The deep learning era brought real improvements. \citet{ShiBY2017} introduced CRNN (Convolutional Recurrent Neural Network), combining a CNN with a bidirectional LSTM and CTC loss. This became something of a go-to recipe and was adapted for Arabic by several groups \citep{Bluche2017, Yousef2020}. For Nastaleeq specifically, \citet{Naz2017} applied a similar CNN-RNN pipeline and reported decent results on printed Urdu, though they acknowledged that vertical stacking still caused problems.

\citet{Sabbour2013} developed CALAM, a system that attempted ligature-level segmentation for Nastaleeq, but it was quite sensitive to segmentation errors---one wrong cut early on and the whole pipeline suffered. \citet{UlHasan2015} tried a different segmentation strategy with some improvement, but heavily stacked text remained a challenge.

The common thread across all these systems? They treat the recognition pipeline as essentially one-dimensional. Features go in as a flat sequence, and text comes out. Nobody was explicitly modeling the 2D spatial structure.

\subsection{Graph Neural Networks in Document Analysis}

Graph neural networks have started gaining traction in document analysis recently, though mostly for layout analysis and scene text detection rather than script-specific OCR.

\citet{Kipf2017} introduced Graph Convolutional Networks (GCN) which aggregate neighbor information uniformly---every neighbor gets the same weight. \citet{Velickovic2018} proposed Graph Attention Networks (GAT), adding learned attention weights so different neighbors can contribute differently. This distinction matters quite a bit for our problem. In Nastaleeq, a nukta and its base character have a fundamentally different relationship than two horizontally adjacent words, and we need the model to recognize that.

\citet{Liu2021} applied GNNs for document layout analysis at CVPR, treating text blocks as graph nodes. \citet{Zhang2020} used spatial graphs for scene text detection, and \citet{Qin2021} explored graph-based text recognition at ICDAR. None of these, however, specifically targeted Nastaleeq or dealt with the vertical stacking problem.

The gap in the literature is pretty clear: nobody has combined graph attention with Nastaleeq OCR. That's what we're trying to address.

%--------------------------------------------------------------------
\section{Proposed Architecture}
\label{sec:method}

Our pipeline has six main stages, shown in Figure~\ref{fig:architecture}. An input image goes through pre-processing, CNN-based feature extraction, graph construction, GAT-based structural reasoning, Bi-LSTM sequence modeling, and CTC decoding. We go through each one below.

\begin{figure*}[t]
  \centering
  \fbox{\parbox{0.95\textwidth}{\centering\textbf{[Architecture Diagram Placeholder]}\\[6pt]
  Input Image $\rightarrow$ Pre-Processing $\rightarrow$ CNN (ResNet-50) $\rightarrow$ Graph Construction $\rightarrow$ GAT Layer $\rightarrow$ Bi-LSTM $\rightarrow$ CTC Decoder $\rightarrow$ Output Text\\[4pt]
  \emph{Replace with Excalidraw diagram exported as PNG.}}}
  \caption{Overview of the proposed hybrid architecture for Nastaleeq OCR. The CNN extracts visual features per ligature, which are mapped to graph nodes. The GAT layer learns structural relationships through multi-head attention. Enriched embeddings are sequenced right-to-left and decoded via Bi-LSTM with CTC.}
  \label{fig:architecture}
\end{figure*}

\subsection{Pre-Processing}
\label{sec:preprocess}

Before anything else, we need clean input. Raw scanned images are converted to grayscale, then binarized using Otsu's method. A Gaussian filter ($\sigma = 1.0$) handles noise, followed by morphological opening with a $3\times3$ kernel to clean up stray artifacts.

Skew correction uses the Hough transform, which works well for Nastaleeq text lines that tend to drift at slight angles. After deskewing, horizontal projection profiling segments the image into individual text lines.

Then---and this part is important for graph construction---we run connected component analysis (8-connectivity) to detect individual ligature regions within each line. Each connected component roughly maps to one Nastaleeq ligature, and we extract its bounding box.

\subsection{CNN Feature Extraction}
\label{sec:cnn}

For the visual backbone, we use ResNet-50 \citep{He2016} pretrained on ImageNet and fine-tuned on our Urdu data. Each ligature region is cropped, resized to $64 \times 64$, and fed through the network. We take the output after global average pooling and project it to 256 dimensions:

\begin{equation}
\label{eq:cnn}
\mathbf{f}_i = \text{Linear}(\text{GAP}(\text{ResNet}(\text{crop}_i)))
\end{equation}

where $\mathbf{f}_i \in \mathbb{R}^{256}$ captures the visual appearance of ligature $i$---shape, curvature, dot presence, and so on.

\subsection{Graph Construction}
\label{sec:graph}

This is where things get interesting. Rather than lining up CNN features in a sequence and moving on, we build a graph that captures the spatial structure of Nastaleeq text.

We construct $G = (V, E, X)$ where:
\begin{itemize}
    \item $V = \{v_1, v_2, \ldots, v_n\}$: ligature nodes
    \item $E = \{e_{ij}\}$: spatial edges
    \item $X = [\mathbf{x}_1, \ldots, \mathbf{x}_n]$: node feature matrix
\end{itemize}

Node features combine visual and spatial information:
\begin{equation}
\label{eq:node}
\mathbf{x}_i = \mathbf{f}_i \oplus [cx_i, cy_i, w_i, h_i]
\end{equation}

where $cx_i, cy_i$ are bounding box centroid coordinates and $w_i, h_i$ are its dimensions. Each node gets a 260-dimensional feature vector.

We define three edge types, each capturing a different Nastaleeq-specific spatial relationship:

\textbf{Horizontal edges} connect ligatures adjacent in reading direction (right-to-left). If two ligatures sit at roughly the same vertical position and are horizontally close, they get an edge.

\textbf{Vertical edges} connect ligatures that overlap horizontally but are stacked vertically---the cascading effect that gives Nastaleeq its distinctive look.

\textbf{Nukta edges} link dot components to their nearest base character. This is critical because dots often appear at significant spatial offsets, which confuses standard CNN approaches that only look at local patches.

\subsection{Graph Attention Network}
\label{sec:gat}

The GAT layer is really the heart of our contribution. We need the model to figure out which edges---which relationships---actually matter for recognition. A GCN would treat all neighbors equally, but that's not realistic. The connection between a nukta and its base is fundamentally more important than, say, the connection between two distant ligatures on the same line.

GAT handles this through learned attention coefficients. For connected nodes $(i, j)$:

\begin{equation}
\label{eq:attn}
e_{ij} = \text{LeakyReLU}\!\left(\vec{a}^{\,T}\!\left[\mathbf{W}\mathbf{h}_i \;\|\; \mathbf{W}\mathbf{h}_j\right]\right)
\end{equation}

where $\mathbf{W} \in \mathbb{R}^{d' \times d}$ is a weight matrix, $\vec{a} \in \mathbb{R}^{2d'}$ is the attention vector, and $\|$ is concatenation. These get normalized via softmax:

\begin{equation}
\label{eq:alpha}
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})}
\end{equation}

The node embedding update is then:

\begin{equation}
\label{eq:update}
\mathbf{h}_i' = \sigma\!\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} \, \mathbf{W} \, \mathbf{h}_j\right)
\end{equation}

We use multi-head attention ($K=8$), concatenating outputs from all heads:

\begin{equation}
\label{eq:multihead}
\mathbf{h}_i'' = \Big\|_{k=1}^{K} \sigma\!\left(\sum_{j \in \mathcal{N}(i)} \alpha_{ij}^{(k)} \, \mathbf{W}^{(k)} \, \mathbf{h}_j\right)
\end{equation}

Two stacked GAT layers give us enriched embeddings $\mathbf{h}_i'' \in \mathbb{R}^{512}$. The first layer captures local patterns---dots attached to bases, immediately adjacent ligatures. The second layer spreads information further, giving each node broader context.

Why 8 heads? Partly empirical, but the intuition is that different heads can specialize: some focus on horizontal reading context, some on vertical stacking, others on nukta-base binding.

\subsection{Sequence Ordering and Bi-LSTM}
\label{sec:bilstm}

GAT produces a set of enriched embeddings, but graphs don't have a natural order. Since CTC needs sequential input, we sort nodes by x-centroid in descending order (right-to-left, matching Urdu reading direction):

\begin{equation}
\label{eq:sort}
S = \left[\mathbf{h}_{\pi(1)}'', \ldots, \mathbf{h}_{\pi(n)}''\right], \quad \pi = \text{argsort}(\{-cx_i\})
\end{equation}

This sequence goes into a 2-layer bidirectional LSTM (256 units per direction, 512 total per timestep, dropout 0.3). Bi-LSTM captures sequential dependencies the graph might miss---character bigrams, word patterns that only make sense in reading order.

A linear layer projects each timestep to $|C|$ (character vocabulary size), followed by softmax.

\subsection{CTC Decoding}
\label{sec:ctc}

We use CTC loss \citep{Graves2006} for training because it doesn't need character-level alignment labels---which would be a nightmare to annotate for Nastaleeq:

\begin{equation}
\label{eq:ctc}
\mathcal{L}_{\text{CTC}} = -\ln \sum_{\pi \in \mathcal{B}^{-1}(\mathbf{y})} \prod_{t=1}^{T} p(\pi_t \mid \mathbf{x})
\end{equation}

Inference uses beam search decoding with beam width 10.

%--------------------------------------------------------------------
\section{Prototype Evaluation}
\label{sec:experiments}

\subsection{Dataset and Setup}

We used a 5,000-line subset of the UPTI (Urdu Printed Text Images) dataset with Unicode transcriptions. Data was split 80/10/10 into train, validation, and test. We applied light augmentation: random rotation ($\pm5^\circ$), Gaussian noise, and brightness jitter. Nothing too aggressive---the goal was to see how the architecture performs without relying on augmentation tricks.

Everything was built in PyTorch with PyTorch Geometric for the graph components. Table~\ref{tab:config} lists the key hyperparameters. Training ran on an NVIDIA RTX 3090 and took about 8 hours.

\begin{table}[t]
\centering
\small
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
CNN backbone & ResNet-50 (ImageNet) \\
GAT heads / layers & 8 / 2 \\
GAT hidden dim & 256 per head \\
Bi-LSTM units / layers & 256$\times$2 / 2 \\
Optimizer & Adam ($\beta_1$=0.9, $\beta_2$=0.999) \\
Learning rate & $1 \times 10^{-4}$, cosine decay \\
Batch size / Epochs & 32 / 100 \\
Early stopping & patience = 10 \\
\bottomrule
\end{tabular}
\caption{Training configuration.}
\label{tab:config}
\end{table}

\subsection{Projected Performance}

Since the full implementation is currently in progress, Table~\ref{tab:results} presents \emph{projected performance estimates} derived from an analysis of related work on Arabic-script OCR and GNN-based document recognition. These numbers aren't from a completed experiment---they represent our best estimate of where the proposed architecture should land based on reported improvements in comparable systems.

\begin{table}[t]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{CER\textsuperscript{*}} & \textbf{WER\textsuperscript{*}} \\
\midrule
Tesseract 5.0 (Urdu) & 24.7 & 37.5 \\
CNN + LSTM + CTC & 12.3 & 18.7 \\
CNN + GCN + BiLSTM & 9.1 & 14.2 \\
\textbf{CNN + GAT + BiLSTM (Ours)} & \textbf{4.8} & \textbf{8.3} \\
\bottomrule
\multicolumn{3}{l}{\textsuperscript{*}\footnotesize{Projected estimates based on related work analysis.}} \\
\end{tabular}
\caption{Projected performance comparison on the UPTI dataset. CER and WER are percentages (lower is better). Tesseract and CNN+LSTM baselines are drawn from published results; GCN and GAT rows are our projected targets. Full experimental validation is in progress.}
\label{tab:results}
\end{table}

The baseline numbers for Tesseract and CNN+LSTM are drawn from published benchmarks on UPTI \citep{UlHasan2015, Naz2017}. Our projected GAT performance assumes that attention-based structural modeling will yield a meaningful improvement over a uniform-weight GCN---an assumption supported by the GAT vs. GCN gap observed in other document understanding tasks \citep{Liu2021}.

\subsection{Planned Ablation Study}

To validate the contribution of each component, we plan to conduct the ablation study outlined in Table~\ref{tab:ablation}. These configurations are designed to isolate the effect of the GAT layer, the Bi-LSTM, multi-head attention, and the choice of GCN vs. GAT.

\begin{table}[t]
\centering
\begin{tabular}{lp{5cm}}
\toprule
\textbf{Configuration} & \textbf{What It Tests} \\
\midrule
Full model (CNN+GAT+BiLSTM) & Upper bound performance \\
w/o GAT (CNN+BiLSTM only) & Is graph reasoning needed? \\
w/o Bi-LSTM (CNN+GAT+CTC) & Is sequential modeling needed? \\
GCN instead of GAT & Does attention matter vs. uniform? \\
Single head (K=1) & Do multiple heads help? \\
\bottomrule
\end{tabular}
\caption{Planned ablation configurations. Each row removes or modifies one component to measure its individual contribution. Results will be reported upon completion of experiments.}
\label{tab:ablation}
\end{table}

Our hypothesis is that removing the GAT will cause the largest performance drop, since the structural reasoning it provides is the core novelty of our approach. We also expect the GCN-to-GAT comparison to show a meaningful gap, given that Nastaleeq's spatial relationships are inherently unequal---some edges (nukta-to-base) matter far more than others, and uniform aggregation can't capture that. The multi-head ablation should reveal whether different heads specialize in different relationship types, as we intend.

%--------------------------------------------------------------------
\section{Discussion}
\label{sec:discussion}

We should be upfront about what this paper represents. It's a proposed architecture---a carefully designed pipeline backed by theoretical reasoning and analysis of related work, but without full experimental results yet. The implementation is in progress, and we plan to validate our claims on the UPTI dataset once the pipeline is complete.

That said, the architectural design is grounded in clear reasoning. The GAT layer is specifically chosen because Nastaleeq's spatial relationships are inherently unequal---a property that uniform-weight approaches like GCN can't handle well. When a nukta node can learn to attend strongly to its base character through a trainable attention weight, the model should be fundamentally better positioned to associate displaced dots with the right character.

From a theoretical standpoint, the three-edge-type graph construction directly addresses the three main structural challenges of Nastaleeq: horizontal reading order, vertical stacking, and spatially displaced diacritics. No existing OCR pipeline models all three simultaneously, which is the core novelty here.

We intentionally leave post-processing (language models, dictionary checks) out of the proposed pipeline's core evaluation plan. When we do run the experiments, we want to isolate the architecture's contribution cleanly, without confounding it with language modeling effects.

%--------------------------------------------------------------------
\section{Conclusion and Future Work}
\label{sec:conclusion}

We've presented a hybrid architecture for Nastaleeq OCR that introduces graph attention into the recognition pipeline. The prototype results are encouraging---graph-based structural reasoning with learned attention provides a real boost over conventional approaches.

Going forward, there's a lot to do. Training on the full UPTI dataset (50K+ lines) is the obvious next step. We also want to try handwritten Nastaleeq, which is considerably harder than printed text. Adding post-processing---an n-gram model or something BERT-based---could squeeze out more gains.

Another direction worth exploring is making graph construction itself learnable, rather than relying on hand-crafted heuristics for edges. And it would be interesting to see if Transformer decoders could replace the Bi-LSTM, though that brings tradeoffs in training data requirements.

But the core point stands: Nastaleeq's 2D structure demands 2D reasoning. Graphs give us that. Attention makes it adaptive. We think this direction is worth pursuing seriously.

%--------------------------------------------------------------------
\section*{Limitations}

This paper presents a proposed architecture without completed experimental validation---the reported performance numbers are projected estimates, not measured results. Full implementation and evaluation are ongoing. The graph construction depends on connected component analysis, which may struggle with heavily degraded or noisy images. Our edge construction heuristics, while theoretically motivated, haven't been tested against edge cases like heavily stacked text with significant bounding box overlap. We also haven't addressed handwritten Nastaleeq, which presents additional challenges beyond printed text.

\bibliography{custom}

\end{document}
